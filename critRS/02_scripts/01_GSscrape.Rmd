---
title: "critRS_GSscrape"
author: "Janice Kai Chen"
date: "4/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message = FALSE}
# loading libraries
library(rvest)
library(dplyr)
library(magrittr)
library(textutils)
```

I will first test out scraping Google Scholar with Alvarez León & Gleason (2017). Here, I manually retrieve the citations link for this article, and use the classes ".gt_rt" and ".gt_a" to retrieve titles, authors, and dates. I will also use retrieve the links to each of the articles.
I will first test out scraping Google Scholar with Alvarez León & Gleason (2017). Here, I manually retrieve the citations link for this article, and use the classes ".gt_rt" and ".gt_a" to retrieve titles, authors, and dates. I will also use retrieve the links to each of the articles.

```{r titles_authors}
url <- 'https://scholar.google.com/scholar?cites=4732551477910239954&as_sdt=5,30&sciodt=0,30&hl=en'

# read in HTML of webpage
article <- read_html(url)

# SelectorGadget indicates titles are housed in an element with the class "gs_rt". Using this class to retrive titles, and so on for author-dates and links. 

# Scraping titles includes the cited article, in this case Alvarez León & Gleason (2017). Keeping this here because can create edge list in this manner. 
titles <- article %>%
  html_nodes(".gs_rt") %>%
  html_text() 


authordate <- article %>%
  html_nodes(".gs_a") %>%
  html_text() 

# inserting NA row in first row to match number of rows in other vectors
t <- NA
authordate <- append(t, authordate)

links <- article %>%
  html_nodes(".gs_rt a") %>%
  html_attr("href") 
```

Next, I will combine these character vectors into a data frame, and then parse out the year for each article. 

```{r dataframe}

# create dataframe, detect & append dates, retrieve index for end of author string, create author column
article_comp <- data.frame(titles, authordate, links, stringsAsFactors = FALSE) %>%
  mutate(year = regmatches((.)$authordate, gregexpr("\\d{4}", (.)$authordate)), 
         index = as.numeric(regexpr("-", (.)$authordate)),
         author = substr(authordate, 1, index-1)) %>%
  # removing "authordate" and "index" columns 
  select(-c(authordate, index))
```

Now I know it is possible to retrive author, date, and link data from a Google Scholar search, I will import the citation list from Bennett (2020), and scrape each article for instances of when they have been cited. 

```{r bennett}
bennett <- read.table("../03_data/01_bennett_citations.tsv", sep = "\t", header = TRUE, stringsAsFactors = FALSE) %>%
  select(1) 
  
# encoding to UTF-8
bennett$citation %<>% enc2utf8()

# adding new column of percent-encoded search string
bennett$searchstring <- lapply(bennett$citation, URLencode)

# storing "base" search string in variable
initstring <- "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C30&q="
```

Next, I need to read the html files of all of these pages and retrive the url that leads to the "Cited By" page. The "Cited by #" link is stored under a div element with the class "gs_fl".The <a> element containing the "Cited by #" is the fourth node in the div element, and the link is stored in the "href" attribute.

```{r citedby}
# THIS CODE DOES NOT WORK (API LIMITS REACHED)
# lapply(bennett$searchstring[1], function(x) {
#   a <- read_html(paste0(initstring, x)) %>%
#     html_nodes(".gs_fl a")
#   
#   b <- a[4] %>%
#     html_attr("href")
#   
#   append(t, b)
# })
# 
# bennett$citedby <- t
```